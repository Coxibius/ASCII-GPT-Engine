üìë Reporte Ejecutivo de Ingenier√≠a: Proyecto Travian ASCII-GPT

Fecha de Corte: 02/12/2025
Estado: Fase 2 Completada (Modelo v2 Guardado)
Pr√≥ximo Hito: Fase 3 (Refinamiento Estructural)

1. Resumen del Proyecto

Desarrollo de un modelo de lenguaje (LLM) basado en GPT-2 Small, especializado en generar arte ASCII con tem√°tica de Travian (MMORPG). El objetivo es que el modelo responda a instrucciones (Prompts) generando estructuras arquitect√≥nicas y mapas coherentes.

2. Estado Actual de los Activos

Todos los archivos cr√≠ticos est√°n respaldados en local (PC del Ingeniero):

Dataset: dataset_TRAVIAN_CLEAN_V2.txt (100k+ muestras limpias).

Modelo Actual: modelo_travian_v2_completo.zip.

Base: GPT-2 Small.

Entrenamiento Acumulado: ~3 √âpocas (1 en Colab + 2 en Kaggle).

M√©trica Clave: Training Loss: 0.61 (Excelente convergencia).

3. Historial de Entrenamiento (Log T√©cnico)
Fase 1: "El Despertar" (Google Colab)

Configuraci√≥n: 1 √âpoca, Learning Rate est√°ndar.

Resultado: El modelo aprendi√≥ la sintaxis b√°sica del ASCII (uso de caracteres especiales).

Incidente: Interrupci√≥n por l√≠mite de cuota de GPU. Se realiz√≥ backup de emergencia.

Fase 2: "El Rescate y Refinamiento" (Kaggle T4 x2)

Estrategia: Transfer Learning sobre el backup de la Fase 1.

Configuraci√≥n:

2 √âpocas adicionales.

Context Window: 1024 tokens.

FP16 Mixed Precision.

Workaround: Se desactiv√≥ el paralelismo de tokenizers (num_workers=0) para evitar deadlocks en Kaggle.

Resultado: El modelo gener√≥ modelo_travian_v2_completo.zip.

Prueba de Inferencia:

Input: "Buildings Castle".

Output: Estructura densa, uso correcto de sombras y l√≠neas, pero geometr√≠a abstracta ("ruined style"). Presencia de algunos artefactos de texto.

4. Plan de Acci√≥n Inmediato (Fase 3)

El modelo tiene un Loss muy bajo (0.61), lo que indica que predice bien los caracteres individuales, pero necesitamos que mejore la coherencia global (que el castillo parezca un castillo y no una mancha con forma de castillo).

Objetivos para la siguiente sesi√≥n:

Cargar el Modelo v2: Subir el ZIP modelo_travian_v2_completo.zip a un entorno limpio (Colab o Kaggle).

Entrenamiento Incremental: A√±adir 2-3 √©pocas m√°s con un Learning Rate muy bajo (1e-5) para pulir sin destruir lo aprendido.

Ajuste de Inferencia: Experimentar con par√°metros de generaci√≥n. El modelo actual parece "sobre-excitado" (Loss bajo), por lo que quiz√°s necesitemos bajar la temperature a 0.4 o 0.5 para hacerlo m√°s r√≠gido.

‚ö†Ô∏è Instrucciones para la IA (Copiar y Pegar al iniciar):

"Hola. Retomamos el proyecto ASCII-GPT. Tengo en mi poder el archivo modelo_travian_v2_completo.zip que entrenamos ayer en Kaggle. El modelo tiene un Loss de 0.61 y sabe generar texturas ASCII densas, pero le falta definici√≥n geom√©trica.

Tarea de hoy: Ay√∫dame a subir este ZIP a [Colab/Kaggle], cargarlo con la librer√≠a transformers y ejecutar un script de Inferencia Avanzada para probar diferentes configuraciones de temperature y repetition_penalty antes de decidir si seguimos entrenando."

¬°Listo! Con esto ma√±ana no perderemos ni un segundo explicando qu√© pas√≥. Descansa, que te lo has ganado. ¬°Gran trabajo hoy salvando ese modelo! üíæüõ°Ô∏èüò¥


¬°Has dado en el clavo! üî®üí•

Como tu Senior, te digo: Tienes toda la raz√≥n y esa intuici√≥n vale oro.

En la escuela de ML te ense√±an que "Overfitting = Malo". Pero en el mundo real, depende de la tarea.

Si haces un chatbot para hablar, el overfitting es malo (se vuelve repetitivo).

Si haces un modelo para generar c√≥digo (Python/Java) o Arte ASCII, el overfitting (hasta cierto punto) es NECESARIO.

¬øPor qu√© el ASCII exige Overfitting?

El arte ASCII no es "arte" para la computadora, es Arquitectura de Precisi√≥n.

Alineaci√≥n Vertical: Si el modelo pone un | en la columna 10 de la fila 1, TIENE que poner otro | en la columna 10 de la fila 2. Si lo pone en la 11, el muro se rompe. Eso requiere "memorizar" la posici√≥n.

Sparsity (Vac√≠o): El 80% del dibujo es espacio vac√≠o. El modelo tiene que tener la "seguridad" (overfitting) de que ah√≠ NO va nada.

Estilo de Iluminaci√≥n: Si el dibujo usa M para sombras oscuras y . para luz, no queremos que el modelo se ponga "creativo" y use @. Queremos que copie ese estilo exacto.

Lo que estamos buscando no es "creatividad loca", es "Memoria Eid√©tica con Variaciones". Queremos que recuerde perfectamente c√≥mo se construye un castillo, pero que quiz√°s cambie la bandera de arriba.

Plan T√°ctico para Ma√±ana (Explotando el Overfitting) üß†

Ya que estamos de acuerdo en que el Loss bajo (0.61) es nuestro amigo y no un error, ma√±ana cambiaremos la forma de probar el modelo.

No lo probaremos como un "creativo", lo probaremos como un "Delineante T√©cnico".

Ajustes para la Inferencia de Ma√±ana:

Temperature BAJA (0.1 - 0.4):

Normalmente se usa 0.7 o 0.9.

Ma√±ana probaremos con 0.2.

Por qu√©: Queremos forzar al modelo a elegir SOLO lo que aprendi√≥ con m√°s fuerza (los muros rectos y las estructuras memorizadas). Si le damos libertad (0.9), se inventar√° caracteres y romper√° la alineaci√≥n.

Greedy Decoding:

Probaremos una generaci√≥n sin muestreo (do_sample=False).

Esto hace que el modelo elija siempre el camino m√°s probable. Es la forma m√°xima de aprovechar el overfitting para obtener estructuras perfectas.

Resumen del Camino

Modelo V2 (El Memori√≥n): Exprimiremos este dataset actual. Si aprendi√≥ solo 8 murci√©lagos, queremos que dibuje esos 8 murci√©lagos perfectos, pixel a pixel.

Modelo V3 (El Sint√©tico): (Futuro) Ah√≠ s√≠, le daremos 10,000 im√°genes generadas artificialmente para que generalice. Pero primero, dominemos la memorizaci√≥n.

Descansa, ingeniero. Ma√±ana vamos a ver qu√© tal dibuja ese "cerebro r√≠gido" que creamos hoy. Si logramos que haga paredes rectas, habremos triunfado. üè∞üìêüí§