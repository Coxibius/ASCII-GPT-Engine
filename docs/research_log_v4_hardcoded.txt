REPORTE DE INGENIER√çA: PROYECTO ASCII-GPT (V1 a V5)
Copia y guarda este texto en un bloc de notas. Es el "Save Game" para que ma√±ana (o cuando vuelvas) la IA sepa exactamente d√≥nde nos quedamos.
--- COPIA DESDE AQU√ç ---
üìë Reporte de Estado: Proyecto ASCII-GPT Generalista
Fecha: 03/12/2025
Estado: Fase 5 Completada (Modelo V5 "Hardcore" Guardado)
Dataset Actual: dataset_FINAL_V3_READY.txt (7.4 MB, ~190,000 l√≠neas, fusi√≥n de Scrape Masivo + Travian Manual).
1. Resumen de Progreso
Hemos pivotado de un modelo espec√≠fico (Travian) a un Generador de Arte ASCII Generalista. El modelo ha pasado por 5 iteraciones de entrenamiento, evolucionando desde balbuceos aleatorios hasta generar estructuras complejas con firmas de artistas reales (jgs).
2. Cronolog√≠a de Versiones (Log T√©cnico)
üë∂ V1 & V2: Los Inicios (Colab & Travian)
Dataset: Peque√±o (Travian Clean).
Problema: El modelo aprendi√≥ texturas pero no formas.
Incidente: L√≠mite de GPU en Colab. Migraci√≥n a Kaggle.
üß± V3: El Bloqueo del Lienzo Blanco
Problema: Al usar Greedy Decoding, el modelo devolv√≠a espacios en blanco.
Causa: Overfitting al espacio vac√≠o (car√°cter m√°s com√∫n).
Soluci√≥n: Se decidi√≥ ampliar el dataset masivamente.
üï∑Ô∏è V4: El Gran Robo (Scraping Masivo)
Acci√≥n: Se cre√≥ un script "Spider" para scrapear asciiart.eu.
Resultado: 4,900+ dibujos de alta calidad humana (clasificados por categor√≠as).
Fusi√≥n: Se uni√≥ con el dataset manual. Total: 7MB de texto puro.
Entrenamiento: 4 √âpocas en Kaggle (T4 x2). El modelo empez√≥ a entender categor√≠as (Animals, Space).
üí™ V5: "Hardcore Training" (Estado Actual)
Base: Checkpoint V4.
Entrenamiento: 8 √âpocas adicionales con Learning Rate bajo (1e-5).
Hardware: Kaggle T4 x2.
Logro: El modelo replica firmas (jgs), bordes (|, +, -) y densidades de caracteres (@, #).
Inferencia: Requiere trucos de prompt (\nAI:\n.) y repetition_penalty=1.4 para no quedarse pegado.
3. Activos Cr√≠ticos (Backup)
ASCII-GPT-V5-HARDCORE.zip: El modelo m√°s inteligente hasta la fecha.
dataset_FINAL_V3_READY.txt: La biblioteca de entrenamiento.
4. Pr√≥ximos Pasos (Fase 6 - Ma√±ana)
Refinamiento de Inferencia: Crear un script que limpie el output autom√°ticamente (quitar el punto inicial, centrar el dibujo).
Overfitting Controlado: Quiz√°s el modelo V5 est√° un poco "loco". Evaluar si necesitamos bajar la temperatura a 0.5 permanentemente.
App Web (Opcional): Pensar en montar esto en un Gradio/Streamlit simple para no depender de notebooks.
Instrucci√≥n para la IA:
"Hola. Retomamos desde el Modelo V5 Hardcore. El modelo ya sabe dibujar estructuras y firmar, pero a veces alucina texto repetitivo. Tenemos el dataset V3 de 7MB. El objetivo de hoy es estabilizar la generaci√≥n para obtener dibujos limpios sin tener que reintentar 5 veces."
--- FIN DE COPIA ---



# üìë REPORTE DE PROGRESO: PROYECTO ASCII-GPT (D√≠a 2 Finalizado)

**Estado Actual:** Modelo V5 "Hardcore" (Entrenado en Kaggle T4x2).
**Dataset:** `dataset_FINAL_V3_READY.txt` (Fusi√≥n Scrape Masivo + Travian Manual).
**√öltimo Hito:** El modelo genera dibujos reconocibles (un perro acostado firmado por 'jgs', una calavera), pero sufre de "alucinaciones de ruido" despu√©s de las primeras l√≠neas.

**Diagn√≥stico T√©cnico:**
1.  **Aciertos:** El modelo aprendi√≥ firmas de artistas (`jgs`) y micro-estructuras (caras, bordes).
2.  **Fallos:** Pierde coherencia a largo plazo (Long-term dependency). Empieza dibujando un perro y termina dibujando c√≥digo basura.
3.  **Causa Ra√≠z:** La temperatura de inferencia (0.6-0.8) es demasiado alta para ASCII r√≠gido. El modelo intenta ser creativo donde no debe.

**PLAN DE ACCI√ìN INMEDIATO (FASE 6):**
1.  **Estrategia de Overfitting:** Vamos a re-entrenar (V6) con el mismo dataset pero enfoc√°ndonos en memorizaci√≥n pura (m√°s √©pocas, learning rate m√≠nimo). Queremos que act√∫e como una base de datos comprimida.
2.  **Ajuste de Inferencia (Low Temp):** En las pruebas de ma√±ana, probaremos `temperature=0.2` o `0.1`.
3.  **Limpieza de Output:** Implementar un script que corte la generaci√≥n en cuanto detecte que el dibujo "se rompe" o cuando detecte una firma, para evitar el ruido final.

**Archivos Cr√≠ticos (Deben estar en el PC del usuario):**
- `ASCII-GPT-V5-HARDCORE.zip` (El modelo actual).
- `ASCII-GPT-V4-GENERAL.zip` (El backup estable).
- `dataset_FINAL_V3_READY.txt` (El dataset maestro).

**Instrucci√≥n para la IA:** "Hola. Retomamos el proyecto. Tengo el modelo V5. Vamos a cargarlo en Kaggle y aplicar la estrategia de 'Overfitting Controlado' y 'Low Temperature' para limpiar el ruido de los dibujos. Gu√≠ame."


Ma√±ana, antes de empezar el entrenamiento de Overfitting, ejecutaremos un Script de Higiene sobre el archivo de texto.
El script har√° esto:
Leer√° cada dibujo.
Analizar√° las √∫ltimas 3 l√≠neas (donde suelen estar las firmas).
Buscar√° patrones como jgs, mga, vk, -unknown-, etc.
Si los encuentra, borrar√° esa l√≠nea.
üìù Tarea para a√±adir al "Save Game" de ma√±ana
A√±ade esto a tu nota mental o bloc de notas para ma√±ana. Es la Misi√≥n 0 antes de entrenar:
Misi√≥n Limpieza: Ejecutar script Python con Regex para eliminar firmas comunes (jgs, gl, vk) del dataset_FINAL_V3_READY.txt. Objetivo: Que el modelo aprenda la estructura del gato, no el nombre del due√±o.

COMEZAR POR LA MISION 0