üìë Reporte Ejecutivo de Ingenier√≠a: Proyecto ASCII-GPT
Fecha de Corte: 04/12/2025 (Cierre de Jornada)
Estado: Fase 6 Completada (Modelo V6 "Clean" Entrenado y Guardado)
Pr√≥ximo Hito: Fase 7 (Benchmarking V5 vs V6 y Despliegue)
1. Resumen de Logros del D√≠a
Se complet√≥ exitosamente el ciclo de Saneamiento de Datos y Reentrenamiento. El objetivo principal era eliminar el sesgo detectado en la versi√≥n V5, donde el modelo replicaba firmas de artistas (jgs, vk) y generaba ruido textual al final de los dibujos. Adem√°s, se profesionaliz√≥ la infraestructura del proyecto (MLOps).
2. Detalles T√©cnicos (Changelog)
üõ†Ô∏è Ingenier√≠a de Datos (Data Sanitization)
Problema: El dataset V3 conten√≠a miles de firmas que causaban alucinaciones de texto.
Soluci√≥n: Ejecuci√≥n local de signature_sanitizer.py (script propio con Regex).
Resultado: Se eliminaron 2,130 firmas (~23% de los finales de bloque).
Nuevo Activo: dataset_v3_combined_clean.txt.
üß† Entrenamiento del Modelo V6 ("The Clean One")
Base: Ascii-art-v5 (Transfer Learning).
Configuraci√≥n:
Hardware: Kaggle GPU T4 x2.
Duraci√≥n: ~8 Horas (25 √âpocas).
Hiperpar√°metros: Learning Rate 1e-5 (Bajo/Estable), Batch Size 8.
M√©tricas Finales: Loss final oscilando entre 0.98 y 1.00.
Interpretaci√≥n: El Loss no baj√≥ a 0.2 (como en V3) porque el modelo ya no memoriza firmas est√°ticas, sino que est√° aprendiendo la estructura general sin el "muleta" de la firma. Esto sugiere una mejor generalizaci√≥n.
‚öôÔ∏è Infraestructura y Repositorio (GitHub)
Se reestructur√≥ el repositorio ASCII-GPT-Engine bajo est√°ndares internacionales.
Documentaci√≥n: README.md en Ingl√©s T√©cnico, estructura de carpetas (notebooks, scripts, docs).
Control de Versiones: Implementaci√≥n de .gitignore y uso de GitHub Releases para distribuir los binarios pesados (V1, V4, V5).
3. Incidentes y Resoluciones
Incidente: Interfaz de Kaggle congelada al finalizar el entrenamiento (mouse inactivo).
Resoluci√≥n: Se logr√≥ recuperar el archivo ASCII-GPT-V6-CLEAN.zip mediante descarga directa del navegador (lenta pero segura) y t√©cnicas de navegaci√≥n por teclado, evitando la p√©rdida de 8 horas de c√≥mputo.
4. Plan de Acci√≥n Inmediato (Fase 7 - Ma√±ana)
El modelo V6 est√° guardado, pero a√∫n no sabemos "c√≥mo dibuja".
Pruebas de Inferencia (CPU): Cargar V6 en un entorno gratuito y ejecutar la matriz de prompts (Animals Cat, Buildings Castle).
KPI de √âxito: Ausencia total de firmas jgs/vk y cierre limpio de las figuras.
Ajuste de Temperatura: Determinar si el modelo limpio necesita una temperatura m√°s alta (0.7) o m√°s baja (0.2) ahora que no tiene firmas.
Demo Web (Opcional): Si el V6 es exitoso, explorar la creaci√≥n de una interfaz simple (Gradio) para compartirlo.
‚ö†Ô∏è Instrucciones para la IA (Copiar y Pegar ma√±ana):
"Hola. Retomamos el proyecto ASCII-GPT. Ayer completamos el entrenamiento del Modelo V6 (Clean). Tengo el archivo ASCII-GPT-V6-CLEAN.zip seguro en mi PC y en Drive.
Este modelo fue entrenado con el dataset sin firmas para corregir las alucinaciones de texto del V5.
Tarea de hoy: Ay√∫dame a configurar un notebook de Inferencia en CPU (para no gastar cuota) para someter al V6 a una bater√≠a de pruebas. Queremos verificar si dej√≥ de firmar los dibujos y si cierra bien las estructuras."


# üíÄ POST-MORTEM: V6 "The Clean One"

**Resultado:** FALLO PARCIAL.
- Aunque se us√≥ un dataset limpio, el entrenamiento se hizo sobre la base del V5 (que ya estaba contaminado).
- **Consecuencia:** El modelo retuvo "conocimiento fantasma" (firmas, textos raros) que aparec√≠an incluso con filtros de regex.

**Lecci√≥n Aprendida:**
"No puedes limpiar un modelo sobre-entrenado solo con Fine-Tuning suave. Se requiere un 'Hard Reset'."

**üéØ PLAN PARA V7 (La Esperanza):**
1.  **Modelo Base:** GPT-2 Small (Fresh / Sin entrenar en ASCII previo).
2.  **Dataset:** `dataset_v3_combined_clean.txt` (El que limpiamos hoy con el script).
3.  **Estrategia:** Entrenamiento desde cero (Training from Scratch / Full Fine-tuning).
    - Esto garantizar√° que el modelo NUNCA haya visto una firma de 'jgs'.
4.  **Hardware:** Esperar al reset de cuota de Kaggle/Colab.


NUEVA HIP√ìTESIS (V7 MATRIX):
Incorporar "Line Tokens" (numeraci√≥n de l√≠neas) en el dataset de entrenamiento para forzar al modelo a entender la dimensi√≥n vertical y la longitud del dibujo. Esto deber√≠a solucionar el problema de las espadas torcidas y los dibujos infinitos.