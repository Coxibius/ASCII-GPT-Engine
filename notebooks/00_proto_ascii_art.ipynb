{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EAtMgG9nHDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e8ae4d9-8f0b-497d-a3a5-a9cd7be0af05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uAtR1vHq6gn",
        "outputId": "b43a803f-e186-4f24-c83f-5df8fdb1010c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚ùå Error: No encuentro la carpeta. Revisa si le pusiste el nombre exacto 'ASCII-GPT' (may√∫sculas importan).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Montamos el Drive (Igual que conectar el USB)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Entramos a la carpeta que creaste a mano\n",
        "# Nota: En c√≥digo, \"Mi unidad\" siempre se escribe \"MyDrive\"\n",
        "ruta_mis_datos = '/content/drive/MyDrive/ASCII-GPT/scraped'\n",
        "\n",
        "try:\n",
        "    os.chdir(ruta_mis_datos)\n",
        "    print(f\"‚úÖ ¬°Genial! Ahora estamos trabajando dentro de: {os.getcwd()}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: No encuentro la carpeta. Revisa si le pusiste el nombre exacto 'ASCII-GPT' (may√∫sculas importan).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ader2mWrKO6",
        "outputId": "846ffc0d-b3a0-4c5f-9610-31fc8300968a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Conectando con la Base de Datos Maestra en GitHub...\n",
            "‚úÖ Base de datos descargada. Buscando dragones...\n",
            "‚ö†Ô∏è No encontr√© la carpeta exacta, buscando manualmente...\n",
            "‚ùå Error fatal: 'list' object has no attribute 'items'\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "# 1. URL de la Base de Datos Maestra en GitHub (Archivo RAW)\n",
        "GITHUB_DB_URL = \"https://raw.githubusercontent.com/asweigart/asciiartjsondb/master/asciiartdb-asciiarteu.json\"\n",
        "\n",
        "def descargar_dragones_vip():\n",
        "    print(\"üöÄ Conectando con la Base de Datos Maestra en GitHub...\")\n",
        "\n",
        "    try:\n",
        "        # Descargamos el JSON gigante\n",
        "        response = requests.get(GITHUB_DB_URL)\n",
        "        response.raise_for_status() # Asegurarnos de que no hubo error 404\n",
        "        data = response.json()\n",
        "\n",
        "        print(\"‚úÖ Base de datos descargada. Buscando dragones...\")\n",
        "\n",
        "        # El JSON tiene una estructura de categor√≠as. Buscamos 'animals' -> 'dragons'\n",
        "        # A veces la categor√≠a exacta puede variar, buscamos en todo lo que huela a drag√≥n\n",
        "\n",
        "        dragones_encontrados = []\n",
        "\n",
        "        # Exploramos las categor√≠as\n",
        "        if 'animals' in data and 'dragons' in data['animals']:\n",
        "            dragones_encontrados = data['animals']['dragons']\n",
        "\n",
        "        # Si no est√°n ah√≠, buscamos en todo el archivo por si acaso\n",
        "        if not dragones_encontrados:\n",
        "            print(\"‚ö†Ô∏è No encontr√© la carpeta exacta, buscando manualmente...\")\n",
        "            for categoria, subcategorias in data.items():\n",
        "                for sub, dibujos in subcategorias.items():\n",
        "                    if 'dragon' in sub.lower():\n",
        "                        dragones_encontrados.extend(dibujos)\n",
        "\n",
        "        print(f\"üî• ¬°ENCONTRADOS {len(dragones_encontrados)} DRAGONES!\")\n",
        "\n",
        "        # Guardar en archivo\n",
        "        nombre_archivo = \"dataset_dragones_vip.txt\"\n",
        "        with open(nombre_archivo, \"w\", encoding=\"utf-8\") as f:\n",
        "            for i, dragon in enumerate(dragones_encontrados):\n",
        "                # Limpieza: si es muy corto, seguramente es basura\n",
        "                if len(dragon) < 50: continue\n",
        "\n",
        "                f.write(f\"\\n<START_ASCII>\\n\")\n",
        "                f.write(dragon)\n",
        "                f.write(f\"\\n<END_ASCII>\\n\")\n",
        "\n",
        "        print(\"\\n------------------------------------------------\")\n",
        "        print(f\"‚úÖ ¬°MISI√ìN CUMPLIDA! Revisa tu Drive.\")\n",
        "        print(f\"   Archivo guardado: {os.getcwd()}/{nombre_archivo}\")\n",
        "        print(\"------------------------------------------------\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error fatal: {e}\")\n",
        "\n",
        "# Ejecutar\n",
        "descargar_dragones_vip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e4Ptptu3QDV",
        "outputId": "6816f36e-0707-44f1-f5fb-9700443df82f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üöÄ Descargando base de datos...\n",
            "‚úÖ JSON descargado. Analizando estructura...\n",
            "------------------------------------------------\n",
            "üî• ¬°PROCESO TERMINADO!\n",
            "‚úÖ Se han guardado 5355 dibujos.\n",
            "üìÇ Archivo listo en: /content/drive/MyDrive/ASCII-GPT/dataset/dataset_limpio_travian.txt\n",
            "------------------------------------------------\n",
            "--- MUESTRA DE C√ìMO QUED√ì EL TXT (√öltimos caracteres) ---\n",
            "     |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    I   '   I                    \n",
            "                     \\     /                     \n",
            "                      \\   /                      \n",
            "                       \\ /                       \n",
            "<|endoftext|>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Montar Drive (si ya est√° montado, no pasa nada)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Configurar ruta\n",
        "ruta_base = '/content/drive/MyDrive/ASCII-GPT/dataset'\n",
        "os.makedirs(ruta_base, exist_ok=True)\n",
        "archivo_salida = os.path.join(ruta_base, 'dataset_limpio_travian.txt')\n",
        "\n",
        "# 3. Descargar el JSON\n",
        "URL_JSON = \"https://raw.githubusercontent.com/asweigart/asciiartjsondb/master/asciiartdb-asciiarteu.json\"\n",
        "print(\"üöÄ Descargando base de datos...\")\n",
        "\n",
        "try:\n",
        "    response = requests.get(URL_JSON)\n",
        "    response.raise_for_status()\n",
        "    data = response.json()\n",
        "\n",
        "    print(f\"‚úÖ JSON descargado. Analizando estructura...\")\n",
        "\n",
        "    # Lista para guardar lo procesado\n",
        "    entradas_procesadas = 0\n",
        "\n",
        "    with open(archivo_salida, \"w\", encoding=\"utf-8\") as f:\n",
        "\n",
        "        # El JSON de la captura parece ser una LISTA de objetos directamente\n",
        "        if isinstance(data, list):\n",
        "            iterator = data\n",
        "        else:\n",
        "            # Si fuera un diccionario gigante, intentamos aplanar todo en una lista\n",
        "            iterator = []\n",
        "            for key, val in data.items():\n",
        "                if isinstance(val, list): iterator.extend(val)\n",
        "\n",
        "        for item in iterator:\n",
        "            # Extraemos los campos con seguridad (usamos .get por si falta alguno)\n",
        "            dibujo = item.get('art', '')\n",
        "            descripcion = item.get('desc', 'unknown object')\n",
        "            categoria = item.get('category', 'general')\n",
        "\n",
        "            # --- FILTROS PARA ESTILO TRAVIAN ---\n",
        "\n",
        "            # 1. Ignorar dibujos vac√≠os\n",
        "            if not dibujo: continue\n",
        "\n",
        "            # 2. (Opcional) Si quieres estilo Travian, ignora los \"one-line\" muy cortos\n",
        "            # Si solo quieres probar, comenta estas dos l√≠neas siguientes:\n",
        "            if len(dibujo) < 10 and \"\\n\" not in dibujo:\n",
        "                continue\n",
        "\n",
        "            # --- LIMPIEZA Y FORMATO ---\n",
        "\n",
        "            # Convertimos los '\\r\\n' o '\\n' literales en saltos de l√≠nea reales\n",
        "            # A veces vienen como \"\\\\n\" en el string raw\n",
        "            dibujo_real = dibujo.replace('\\\\n', '\\n').replace('\\\\r', '')\n",
        "\n",
        "            # Escribimos en el formato que le gusta a NanoGPT / GPT-2\n",
        "            # Usamos tokens especiales para que sepa cu√°ndo empieza y acaba\n",
        "\n",
        "            f.write(f\"<|startoftext|>\\n\")\n",
        "            f.write(f\"PROMPT: {descripcion}\\n\")\n",
        "            f.write(f\"ASCII:\\n\")\n",
        "            f.write(f\"{dibujo_real}\\n\")\n",
        "            f.write(f\"<|endoftext|>\\n\")\n",
        "            f.write(f\"\\n\") # Separador visual en el txt\n",
        "\n",
        "            entradas_procesadas += 1\n",
        "\n",
        "    print(\"------------------------------------------------\")\n",
        "    print(f\"üî• ¬°PROCESO TERMINADO!\")\n",
        "    print(f\"‚úÖ Se han guardado {entradas_procesadas} dibujos.\")\n",
        "    print(f\"üìÇ Archivo listo en: {archivo_salida}\")\n",
        "    print(\"------------------------------------------------\")\n",
        "\n",
        "    # Imprimimos una muestra para que veas si te gusta el formato\n",
        "    print(\"--- MUESTRA DE C√ìMO QUED√ì EL TXT (√öltimos caracteres) ---\")\n",
        "    with open(archivo_salida, 'r') as f:\n",
        "        print(f.read()[-500:])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyhErgEt3d0u",
        "outputId": "72425e2a-404f-4e95-ed8e-07953863cab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Buscando murci√©lagos (bats) para ver si se ven bien...\n",
            "------------------------------------------------------\n",
            "DESC: No description.\n",
            "TAMA√ëO: 22 x 4\n",
            "DIBUJO:\n",
            "        _   ,_,   _   \n",
            "       / `'=) (='` \\  \n",
            "      /.-.-.\\ /.-.-.\\ \n",
            "jgs   `      \"      ` \n",
            "------------------------------------------------------\n",
            "\n",
            "DESC: No description.\n",
            "TAMA√ëO: 24 x 4\n",
            "DIBUJO:\n",
            "       (_    ,_,    _)  \n",
            "       / `'--) (--'` \\  \n",
            "      /  _,-'\\_/'-,_  \\ \n",
            "jgs  /.-'     \"     '-.\\\n",
            "------------------------------------------------------\n",
            "\n",
            "DESC: No description.\n",
            "TAMA√ëO: 31 x 7\n",
            "DIBUJO:\n",
            "    =/\\                 /\\=    \n",
            "    / \\'._   (\\_/)   _.'/ \\    \n",
            "   / .''._'--(o.o)--'_.''. \\   \n",
            "  /.' _/ |`'=/ \" \\='`| \\_ `.\\  \n",
            " /` .' `\\;-,'\\___/',-;/` '. '\\ \n",
            "/.-' jgs   `\\(-V-)/`       `-.\\\n",
            "`            \"   \"            `\n",
            "------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import random\n",
        "\n",
        "# Descargamos el mismo archivo\n",
        "url = \"https://raw.githubusercontent.com/asweigart/asciiartjsondb/master/asciiartdb-asciiarteu.json\"\n",
        "data = requests.get(url).json()\n",
        "\n",
        "print(\"üîç Buscando murci√©lagos (bats) para ver si se ven bien...\")\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "# Buscamos en la lista\n",
        "encontrados = 0\n",
        "for item in data:\n",
        "    # Filtramos solo los que dicen \"bat\" en la categor√≠a o descripci√≥n\n",
        "    if 'bat' in item.get('category', '').lower() or 'bat' in item.get('desc', '').lower():\n",
        "\n",
        "        art = item['art']\n",
        "\n",
        "        # FILTRO DE CALIDAD:\n",
        "        # Si es muy peque√±o o no tiene saltos de l√≠nea (\\n), lo ignoramos\n",
        "        if len(art) < 20 or '\\n' not in art:\n",
        "            continue\n",
        "\n",
        "        print(f\"DESC: {item.get('desc')}\")\n",
        "        print(f\"TAMA√ëO: {item.get('width')} x {item.get('height')}\")\n",
        "        print(\"DIBUJO:\")\n",
        "        print(art) # <--- EL SECRETO: Al usar print(), Python interpreta los \\n\n",
        "        print(\"------------------------------------------------------\\n\")\n",
        "\n",
        "        encontrados += 1\n",
        "        if encontrados >= 3: break # Solo mostramos 3 para no llenar la pantalla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmRH9Ga74Hrx",
        "outputId": "2ad8baf0-a9a2-4c0e-9715-4942818a2ea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üöÄ Iniciando proceso de conversi√≥n masiva...\n",
            "‚úÖ Base de datos descargada. Total de objetos crudos: 5474\n",
            "------------------------------------------------\n",
            "üî• ¬°PROCESO TERMINADO!\n",
            "‚úÖ Se guardaron 5268 dibujos de alta calidad (Multil√≠nea).\n",
            "üìÇ Archivo guardado en: /content/drive/MyDrive/ASCII-GPT/dataset/dataset_limpio_travian.txt\n",
            "------------------------------------------------\n",
            "üëÄ As√≠ se ve una entrada de ejemplo en tu archivo:\n",
            "<|startoftext|>\n",
            "User: Dibujo ASCII de Art by HeavensRevenge\n",
            "AI:\n",
            "                      _..._                      \n",
            "                     /MMMMM\\                     \n",
            "                    (I8H#H8I)                    \n",
            "                    (I8H#H8I)                    \n",
            "                     \\WWWWW/                     \n",
            "                      I._.I                      \n",
            "                      I._.I                      \n",
            "                      I._.I                      \n",
            "                      I._.I                      \n",
            "                      I._.I                      \n",
            "                      I._.I                      \n",
            "                      I._.I                      \n",
            "                      I.,.I                      \n",
            "                     / /#\\ \\                     \n",
            "                   .dH# # #Hb.                   \n",
            "               _.~d#XXP I 7XX#b~,_               \n",
            "            _.dXV^XP^ Y X Y ^7X^VXb._            \n",
            "           /AP^   \\PY   Y   Y7/   ^VA\\           \n",
            "          /8/      \\PP  I  77/      \\8\\          \n",
            "         /J/        IV     VI        \\L\\         \n",
            "         L|         |  \\ /  |         |J         \n",
            "         V          |  | |  |          V         \n",
            "                    |  | |  |                    \n",
            "                    |  | |  |                    \n",
            "                    |  | |  |                    \n",
            "                    |  | |  |                    \n",
            " _                  |  | |  |                  _ \n",
            "( \\                 |  | |  |                 / )\n",
            " \\ \\                |  | |  |                / / \n",
            "('\\ \\               |  | |  |               / /`)\n",
            " \\ \\ \\              |  | |  |              / / / \n",
            "('\\ \\ \\             |  | |  |             / / /`)\n",
            " \\ \\ \\ )            |  | |  |            ( / / / \n",
            "('\\ \\( )            |  | |  |            ( )/ /`)\n",
            " \\ \\ ( |            |  | |  |            | ) / / \n",
            "  \\ \\( |            |  | |  |            | )/ /  \n",
            "   \\ ( |            |  | |  |            | ) /   \n",
            "    \\( |            |   Y   |            | )/    \n",
            "     | |            |   |   |            | |     \n",
            "     J | ___...~~--'|   |   |`--~~...___ | L     \n",
            "     >-+<...___     |   |   |     ___...>+-<     \n",
            "    /     __   `--~.L___L___J.~--'   __     \\    \n",
            "    K    /  ` --.     d===b     .-- '  \\    H    \n",
            "    \\_._/        \\   // I \\\\   /        \\_._/    \n",
            "      `--~.._     \\__\\\\ I //__/     _..~--'      \n",
            "             `--~~..____ ____..~~--'             \n",
            "                    |   T   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    |   |   |                    \n",
            "                    I   '   I                    \n",
            "                     \\     /                     \n",
            "                      \\   /                      \n",
            "                       \\ /                       \n",
            "<|endoftext|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Conectar Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Preparar carpeta\n",
        "ruta_carpeta = '/content/drive/MyDrive/ASCII-GPT/dataset'\n",
        "os.makedirs(ruta_carpeta, exist_ok=True)\n",
        "archivo_final = os.path.join(ruta_carpeta, 'dataset_limpio_travian.txt')\n",
        "\n",
        "print(\"üöÄ Iniciando proceso de conversi√≥n masiva...\")\n",
        "\n",
        "# 3. Descargar JSON\n",
        "url = \"https://raw.githubusercontent.com/asweigart/asciiartjsondb/master/asciiartdb-asciiarteu.json\"\n",
        "try:\n",
        "    data = requests.get(url).json()\n",
        "    print(f\"‚úÖ Base de datos descargada. Total de objetos crudos: {len(data)}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error descargando: {e}\")\n",
        "    data = []\n",
        "\n",
        "# 4. Procesar y Limpiar\n",
        "conteo = 0\n",
        "with open(archivo_final, \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in data:\n",
        "        art = item.get('art', '')\n",
        "        desc = item.get('desc', 'unknown object')\n",
        "\n",
        "        # --- FILTROS IMPORTANTES ---\n",
        "        # 1. Si no tiene descripci√≥n √∫til, usamos la categor√≠a\n",
        "        if len(desc) < 3 or desc == \"No description.\":\n",
        "            desc = item.get('category', 'object')\n",
        "\n",
        "        # 2. LA REGLA DE ORO: Solo queremos \"Matrices\" (estilo Travian)\n",
        "        # Si el dibujo no tiene saltos de l√≠nea (\\n), es de una sola l√≠nea -> LO BORRAMOS\n",
        "        if '\\n' not in art:\n",
        "            continue\n",
        "\n",
        "        # 3. Si es muy peque√±o (menos de 15 caracteres), suele ser basura -> LO BORRAMOS\n",
        "        if len(art) < 15:\n",
        "            continue\n",
        "\n",
        "        # --- FORMATO DE ENTRENAMIENTO ---\n",
        "        # Esto ense√±a a la IA: \"Si te piden X, dibuja Y\"\n",
        "        prompt_text = f\"Dibujo ASCII de {desc}\"\n",
        "\n",
        "        # Limpiamos caracteres raros del dibujo\n",
        "        art_limpio = art.replace('\\\\n', '\\n').replace('\\\\r', '')\n",
        "\n",
        "        entry = (\n",
        "            f\"<|startoftext|>\\n\"\n",
        "            f\"User: {prompt_text}\\n\"\n",
        "            f\"AI:\\n\"\n",
        "            f\"{art_limpio}\\n\"\n",
        "            f\"<|endoftext|>\\n\"\n",
        "        )\n",
        "\n",
        "        f.write(entry)\n",
        "        conteo += 1\n",
        "\n",
        "print(\"------------------------------------------------\")\n",
        "print(f\"üî• ¬°PROCESO TERMINADO!\")\n",
        "print(f\"‚úÖ Se guardaron {conteo} dibujos de alta calidad (Multil√≠nea).\")\n",
        "print(f\"üìÇ Archivo guardado en: {archivo_final}\")\n",
        "print(\"------------------------------------------------\")\n",
        "print(\"üëÄ As√≠ se ve una entrada de ejemplo en tu archivo:\")\n",
        "print(entry) # Muestra el √∫ltimo procesado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kix2Fup8-m3e",
        "outputId": "858bf445-f68e-4936-d298-6d0f2d2357c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Archivo encontrado. ¬°Iniciando configuraci√≥n!\n",
            "‚öôÔ∏è Usando: cuda (Recomendado: cuda)\n",
            "\n",
            "ü•ä ¬°EMPIEZA EL ENTRENAMIENTO!\n",
            "Paso 0: loss 5.1955\n",
            "\u0003\t√ÑwWvgs~#;.√≠√Ö[√•2hb5:+e√Ö+)¬£$?q√≠RF(V}pL:;√ÖO¬•¬∏SP√Ñf√ñJRx+x6%¬πc`¬£-√Ñu^YLDHK:¬ß$¬∑1^,^U¬® \\k:zi^icPB|\u0003|¬∫:4√Ñ√ú`'\n",
            "--------------------\n",
            "Paso 500: loss 1.0467\n",
            "\u0003   z7$$$hr                    /              $$$$$$                   \n",
            " | |^\\       ---.    `$$$)--!\n",
            "--------------------\n",
            "Paso 1000: loss 0.9709\n",
            "\u0003                            \n",
            "           /        |     (                                     \n",
            "      \n",
            "--------------------\n",
            "Paso 1500: loss 0.9099\n",
            "\u0003 --.     ./   .  \\_) /_|____,-'_. \\   \\___-`___>  __`--\\          \n",
            "_a_______|/  #| |/_/____7__/| _| \n",
            "--------------------\n",
            "Paso 2000: loss 0.8664\n",
            "\u0003 4n`MP ,nnnm4MMMMMn  $$$$$$$$$$$$$$$$$$$$$$$$$$$$hcc,4MMMMMM   \n",
            "  `$$c, xm`$h <?%xn, ?$$$$$$$c??$$$$\n",
            "--------------------\n",
            "Paso 2500: loss 0.8364\n",
            "\u0003aaaaaaaaaaaaaaaa          \n",
            "\n",
            "            /!?\"\"\"\"\"\"\"\"\"'''    <!!!!!!!!!!!!!!!!!!!!!!><!!!!!!!!!!'!!!>!\n",
            "--------------------\n",
            "Paso 3000: loss 0.8102\n",
            "\u0003               \n",
            "                 *g@U!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!:             \n",
            "         \n",
            "--------------------\n",
            "Paso 3500: loss 0.7897\n",
            "\u0003\n",
            "               CC'HHHH    ,<'''                               \n",
            "       .;::\"HHLF   ,d      (   ) .,)\n",
            "--------------------\n",
            "Paso 4000: loss 0.7638\n",
            "\u0003    \n",
            "@@@     \\@@  /@@@@@@@ -  \n",
            "     \\     `-.`@@@@     ,'     \n",
            "    \\_   _ _'  __/`      ___(_\n",
            "      \n",
            "--------------------\n",
            "Paso 4500: loss 0.7610\n",
            "\u0003      :;!       \n",
            "         :$$$$$$$$$$$$$$$P\"        ::::.:'         ;      \n",
            "        :`$$$$$$$$$$$$  \n",
            "--------------------\n",
            "Paso 4999: loss 0.7457\n",
            "\u0003  |  :   : \\   |   /    :  .-.::::::\\(\") :.-.|\n",
            "|   \\. |    .-\"-.\"  /    | \\  |  |\\ .-\".\"-\".-. \\ '-. \n",
            "--------------------\n",
            "üíæ ¬°MODELO GUARDADO EN DRIVE! -> /content/drive/MyDrive/ASCII-GPT/modelo_ascii.pt\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Asegurar que Drive est√° conectado\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# --- RUTA CORREGIDA AQU√ç ---\n",
        "# Apuntamos a la carpeta 'dataset' que creamos antes\n",
        "ruta_archivo = '/content/drive/MyDrive/ASCII-GPT/dataset/dataset_limpio_travian.txt'\n",
        "ruta_modelo = '/content/drive/MyDrive/ASCII-GPT/modelo_ascii.pt'\n",
        "\n",
        "# Verificaci√≥n r√°pida antes de empezar\n",
        "if not os.path.exists(ruta_archivo):\n",
        "    print(f\"‚ùå ERROR: No encuentro el archivo en: {ruta_archivo}\")\n",
        "    print(\"Por favor, verifica en tu Drive si la carpeta se llama 'dataset' o 'scraped'.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Archivo encontrado. ¬°Iniciando configuraci√≥n!\")\n",
        "\n",
        "    # --- CONFIGURACI√ìN DE LA RED NEURONAL ---\n",
        "    batch_size = 64\n",
        "    block_size = 256\n",
        "    max_iters = 5000  # Iteraciones de entrenamiento\n",
        "    eval_interval = 500\n",
        "    learning_rate = 3e-4\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    print(f\"‚öôÔ∏è Usando: {device} (Recomendado: cuda)\")\n",
        "\n",
        "    # Leer datos\n",
        "    with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Tokenizaci√≥n\n",
        "    chars = sorted(list(set(text)))\n",
        "    vocab_size = len(chars)\n",
        "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "    itos = { i:ch for i,ch in enumerate(chars) }\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "    data = torch.tensor(encode(text), dtype=torch.long)\n",
        "    n = int(0.9 * len(data))\n",
        "    train_data = data[:n]\n",
        "    val_data = data[n:]\n",
        "\n",
        "    # Definici√≥n del Modelo (Mini GPT)\n",
        "    n_embd = 384\n",
        "    n_head = 6\n",
        "    n_layer = 6\n",
        "    dropout = 0.2\n",
        "\n",
        "    class Head(nn.Module):\n",
        "        def __init__(self, head_size):\n",
        "            super().__init__()\n",
        "            self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "            self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "            self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        def forward(self, x):\n",
        "            B,T,C = x.shape\n",
        "            k = self.key(x)\n",
        "            q = self.query(x)\n",
        "            wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
        "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "            wei = F.softmax(wei, dim=-1)\n",
        "            wei = self.dropout(wei)\n",
        "            v = self.value(x)\n",
        "            out = wei @ v\n",
        "            return out\n",
        "\n",
        "    class MultiHeadAttention(nn.Module):\n",
        "        def __init__(self, num_heads, head_size):\n",
        "            super().__init__()\n",
        "            self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "            self.proj = nn.Linear(n_embd, n_embd)\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        def forward(self, x):\n",
        "            out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "            out = self.proj(out)\n",
        "            return out\n",
        "\n",
        "    class FeedForward(nn.Module):\n",
        "        def __init__(self, n_embd):\n",
        "            super().__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(n_embd, 4 * n_embd),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(4 * n_embd, n_embd),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.net(x)\n",
        "\n",
        "    class Block(nn.Module):\n",
        "        def __init__(self, n_embd, n_head):\n",
        "            super().__init__()\n",
        "            head_size = n_embd // n_head\n",
        "            self.sa = MultiHeadAttention(n_head, head_size)\n",
        "            self.ffwd = FeedForward(n_embd)\n",
        "            self.ln1 = nn.LayerNorm(n_embd)\n",
        "            self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x + self.sa(self.ln1(x))\n",
        "            x = x + self.ffwd(self.ln2(x))\n",
        "            return x\n",
        "\n",
        "    class GPTLanguageModel(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "            self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "            self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "            self.ln_f = nn.LayerNorm(n_embd)\n",
        "            self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        def forward(self, idx, targets=None):\n",
        "            B, T = idx.shape\n",
        "            tok_emb = self.token_embedding_table(idx)\n",
        "            pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "            x = tok_emb + pos_emb\n",
        "            x = self.blocks(x)\n",
        "            x = self.ln_f(x)\n",
        "            logits = self.lm_head(x)\n",
        "\n",
        "            if targets is None:\n",
        "                loss = None\n",
        "            else:\n",
        "                B, T, C = logits.shape\n",
        "                logits = logits.view(B*T, C)\n",
        "                targets = targets.view(B*T)\n",
        "                loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "            return logits, loss\n",
        "\n",
        "        def generate(self, idx, max_new_tokens):\n",
        "            for _ in range(max_new_tokens):\n",
        "                idx_cond = idx[:, -block_size:]\n",
        "                logits, loss = self(idx_cond)\n",
        "                logits = logits[:, -1, :]\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "                idx = torch.cat((idx, idx_next), dim=1)\n",
        "            return idx\n",
        "\n",
        "    model = GPTLanguageModel()\n",
        "    m = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    def get_batch(split):\n",
        "        data = train_data if split == 'train' else val_data\n",
        "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "        x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "        y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        return x, y\n",
        "\n",
        "    print(\"\\nü•ä ¬°EMPIEZA EL ENTRENAMIENTO!\")\n",
        "\n",
        "    for iter in range(max_iters):\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = {}\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for split in ['train', 'val']:\n",
        "                    losses[split] = torch.zeros(200).to(device)\n",
        "                    for k in range(200):\n",
        "                        X, Y = get_batch(split)\n",
        "                        logits, loss = model(X, Y)\n",
        "                        losses[split][k] = loss.item()\n",
        "                    losses[split] = losses[split].mean()\n",
        "            model.train()\n",
        "            print(f\"Paso {iter}: loss {losses['train']:.4f}\")\n",
        "\n",
        "            # Muestra dibujo de prueba\n",
        "            context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "            print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "        xb, yb = get_batch('train')\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Guardar\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'stoi': stoi,\n",
        "        'itos': itos,\n",
        "        'chars': chars\n",
        "    }, ruta_modelo)\n",
        "    print(f\"üíæ ¬°MODELO GUARDADO EN DRIVE! -> {ruta_modelo}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "from google.colab import drive # Import drive explicitly for force_remount\n",
        "\n",
        "# --- 1. RECONSTRUIR LA ESTRUCTURA DEL CEREBRO ---\n",
        "# (Necesitamos definir la clase igual que antes para cargar los datos)\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits, None\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "            # Si la IA genera el token de fin, paramos para no generar basura extra\n",
        "            # (Esto es opcional, pero queda m√°s limpio)\n",
        "        return idx\n",
        "\n",
        "# --- 2. CARGAR EL MODELO ENTRENADO ---\n",
        "ruta_modelo = '/content/drive/MyDrive/ASCII-GPT/modelo_ascii.pt'\n",
        "print(f\"üìÇ Cargando modelo desde: {ruta_modelo}\")\n",
        "\n",
        "# A√±adir una verificaci√≥n de existencia del archivo y manejar el montaje de Drive\n",
        "if not os.path.exists(ruta_modelo):\n",
        "    print(f\"‚ùå ERROR: ¬°El archivo del modelo NO SE ENCUENTRA en {ruta_modelo}!\")\n",
        "    print(\"Esto puede pasar si el Drive no est√° montado correctamente o si el modelo no se guard√≥ en la ubicaci√≥n esperada.\")\n",
        "    print(\"Aseg√∫rate de que la celda de entrenamiento (anterior) se haya ejecutado COMPLETAMENTE y sin errores.\")\n",
        "    # Intentar forzar el remontaje de Google Drive\n",
        "    print(\"Intentando forzar el remontaje de Google Drive...\")\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    # Volver a verificar despu√©s de remontar\n",
        "    if not os.path.exists(ruta_modelo):\n",
        "        print(f\"‚ùå ERROR CR√çTICO: Incluso despu√©s de remontar, el archivo {ruta_modelo} no se encuentra.\")\n",
        "        print(\"Por favor, verifica manualmente tu Google Drive para asegurarte de que el archivo existe y la ruta es correcta.\")\n",
        "        # Lanzar una excepci√≥n para detener la ejecuci√≥n y alertar al usuario\n",
        "        raise FileNotFoundError(f\"Modelo no encontrado en: {ruta_modelo}. Por favor, verifica tu Drive.\")\n",
        "\n",
        "checkpoint = torch.load(ruta_modelo, map_location=device)\n",
        "stoi = checkpoint['stoi']\n",
        "itos = checkpoint['itos']\n",
        "chars = checkpoint['chars']\n",
        "vocab_size = len(chars)\n",
        "\n",
        "model = GPTLanguageModel(vocab_size)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.to(device)\n",
        "model.eval() # Modo evaluaci√≥n (apaga el aprendizaje)\n",
        "\n",
        "print(\"‚úÖ ¬°Modelo cargado y listo para dibujar!\")\n",
        "\n",
        "# --- 3. FUNCI√ìN PARA GENERAR ---\n",
        "def generar_ascii(prompt_usuario):\n",
        "    print(f\"\\nüé® Pidiendo: '{prompt_usuario}'...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Preparamos el prompt tal cual aprendi√≥ la IA\n",
        "    # Formato: <|startoftext|>\\nUser: Dibujo ASCII de perro\\nAI:\\n\n",
        "    start_prompt = f\"<|startoftext|>\\nUser: Dibujo ASCII de {prompt_usuario}\\nAI:\\n\"\n",
        "\n",
        "    # Convertimos letras a n√∫meros\n",
        "    context_idxs = [stoi.get(c, 0) for c in start_prompt] # Si no conoce un caracter, pone 0\n",
        "    context = torch.tensor([context_idxs], dtype=torch.long, device=device)\n",
        "\n",
        "    # Generamos hasta 1000 caracteres (para que le de tiempo a terminar)\n",
        "    generated_ids = model.generate(context, max_new_tokens=1000)\n",
        "\n",
        "    # Convertimos n√∫meros a letras\n",
        "    resultado = ''.join([itos[i] for i in generated_ids[0].tolist()])\n",
        "\n",
        "    # Limpieza visual: quitamos el prompt inicial para ver solo el dibujo\n",
        "    # y cortamos si aparece el token de final\n",
        "    dibujo = resultado.replace(start_prompt, \"\")\n",
        "    if \"<|endoftext|>\" in dibujo:\n",
        "        dibujo = dibujo.split(\"<|endoftext|>\")[0]\n",
        "\n",
        "    print(dibujo)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# --- 4. ¬°PRUEBA TUS DIBUJOS! ---\n",
        "# Cambia estas palabras por cosas que creas que est√°n en el dataset\n",
        "generar_ascii(\"dragon\")\n",
        "generar_ascii(\"sword\")  # Espada\n",
        "generar_ascii(\"castle\") # Castillo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "087pyRIHK4bH",
        "outputId": "060e0fc1-8489-4e29-9b9f-f5b511a097d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Cargando modelo desde: /content/drive/MyDrive/ASCII-GPT/modelo_ascii.pt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/ASCII-GPT/modelo_ascii.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2806912932.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üìÇ Cargando modelo desde: {ruta_modelo}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruta_modelo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0mstoi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stoi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0mitos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'itos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ASCII-GPT/modelo_ascii.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Asegurar Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Rutas\n",
        "carpeta = '/content/drive/MyDrive/ASCII-GPT'\n",
        "archivo = os.path.join(carpeta, 'modelo_ascii.pt')\n",
        "\n",
        "print(f\"üìÇ Buscando en: {carpeta}\")\n",
        "\n",
        "# 3. VERIFICAR QU√â HAY\n",
        "if os.path.exists(carpeta):\n",
        "    archivos = os.listdir(carpeta)\n",
        "    print(f\"üëÄ Archivos encontrados: {archivos}\")\n",
        "\n",
        "    if 'modelo_ascii.pt' in archivos:\n",
        "        print(\"‚úÖ ¬°El archivo EST√Å ah√≠! Probablemente fue un error moment√°neo de conexi√≥n.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è El archivo NO aparece en la lista.\")\n",
        "else:\n",
        "    print(\"‚ùå La carpeta ASCII-GPT no existe. ¬øQuiz√°s est√° en 'Mi unidad' suelta?\")\n",
        "\n",
        "# 4. INTENTO DE RESCATE (Si acabas de entrenar)\n",
        "# Si las variables del modelo siguen en la memoria RAM, lo guardamos otra vez.\n",
        "print(\"\\nüöë Intentando guardar de nuevo...\")\n",
        "try:\n",
        "    # Verificamos si 'model' existe en la memoria de Python\n",
        "    if 'model' in globals() and 'stoi' in globals():\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'stoi': stoi,\n",
        "            'itos': itos,\n",
        "            'chars': chars\n",
        "        }, archivo)\n",
        "        print(f\"üéâ ¬°√âXITO! Se ha forzado el guardado en: {archivo}\")\n",
        "        print(\"Ahora vuelve a intentar ejecutar el bloque de carga (el anterior).\")\n",
        "    else:\n",
        "        print(\"üí® Las variables de entrenamiento ya no est√°n en memoria.\")\n",
        "        print(\"Si el archivo no aparece arriba, tendr√°s que volver a ejecutar el bloque de 'Entrenamiento' (Sprint 2).\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error al intentar guardar: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRVTv7s_L3ds",
        "outputId": "4186ba9a-d69e-40a6-8e78-0dcacb926e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "üìÇ Buscando en: /content/drive/MyDrive/ASCII-GPT\n",
            "üëÄ Archivos encontrados: ['dataset', 'modelo_ascii.pt']\n",
            "‚úÖ ¬°El archivo EST√Å ah√≠! Probablemente fue un error moment√°neo de conexi√≥n.\n",
            "\n",
            "üöë Intentando guardar de nuevo...\n",
            "üí® Las variables de entrenamiento ya no est√°n en memoria.\n",
            "Si el archivo no aparece arriba, tendr√°s que volver a ejecutar el bloque de 'Entrenamiento' (Sprint 2).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "\n",
        "# --- 1. RECONSTRUIR LA ESTRUCTURA DEL CEREBRO ---\n",
        "# (Necesitamos definir la clase igual que antes para cargar los datos)\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits, None\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "            # Si la IA genera el token de fin, paramos para no generar basura extra\n",
        "            # (Esto es opcional, pero queda m√°s limpio)\n",
        "        return idx\n",
        "\n",
        "# --- 2. CARGAR EL MODELO ENTRENADO ---\n",
        "ruta_modelo = '/content/drive/MyDrive/ASCII-GPT/modelo_ascii.pt'\n",
        "print(f\"üìÇ Cargando modelo desde: {ruta_modelo}\")\n",
        "\n",
        "checkpoint = torch.load(ruta_modelo, map_location=device)\n",
        "stoi = checkpoint['stoi']\n",
        "itos = checkpoint['itos']\n",
        "chars = checkpoint['chars']\n",
        "vocab_size = len(chars)\n",
        "\n",
        "model = GPTLanguageModel(vocab_size)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.to(device)\n",
        "model.eval() # Modo evaluaci√≥n (apaga el aprendizaje)\n",
        "\n",
        "print(\"‚úÖ ¬°Modelo cargado y listo para dibujar!\")\n",
        "\n",
        "# --- 3. FUNCI√ìN PARA GENERAR ---\n",
        "def generar_ascii(prompt_usuario):\n",
        "    print(f\"\\nüé® Pidiendo: '{prompt_usuario}'...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Preparamos el prompt tal cual aprendi√≥ la IA\n",
        "    # Formato: <|startoftext|>\\nUser: Dibujo ASCII de perro\\nAI:\\n\n",
        "    start_prompt = f\"<|startoftext|>\\nUser: Dibujo ASCII de {prompt_usuario}\\nAI:\\n\"\n",
        "\n",
        "    # Convertimos letras a n√∫meros\n",
        "    context_idxs = [stoi.get(c, 0) for c in start_prompt] # Si no conoce un caracter, pone 0\n",
        "    context = torch.tensor([context_idxs], dtype=torch.long, device=device)\n",
        "\n",
        "    # Generamos hasta 1000 caracteres (para que le de tiempo a terminar)\n",
        "    generated_ids = model.generate(context, max_new_tokens=1000)\n",
        "\n",
        "    # Convertimos n√∫meros a letras\n",
        "    resultado = ''.join([itos[i] for i in generated_ids[0].tolist()])\n",
        "\n",
        "    # Limpieza visual: quitamos el prompt inicial para ver solo el dibujo\n",
        "    # y cortamos si aparece el token de final\n",
        "    dibujo = resultado.replace(start_prompt, \"\")\n",
        "    if \"<|endoftext|>\" in dibujo:\n",
        "        dibujo = dibujo.split(\"<|endoftext|>\")[0]\n",
        "\n",
        "    print(dibujo)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# --- 4. ¬°PRUEBA TUS DIBUJOS! ---\n",
        "# Cambia estas palabras por cosas que creas que est√°n en el dataset\n",
        "generar_ascii(\"dragon\")\n",
        "generar_ascii(\"sword\")  # Espada\n",
        "generar_ascii(\"castle\") # Castillo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhtVxcrnMfig",
        "outputId": "e17f88b5-1e67-421a-9abf-c1cdfe448fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Cargando modelo desde: /content/drive/MyDrive/ASCII-GPT/modelo_ascii.pt\n",
            "‚úÖ ¬°Modelo cargado y listo para dibujar!\n",
            "\n",
            "üé® Pidiendo: 'dragon'...\n",
            "----------------------------------------\n",
            "                         /                \n",
            "                 \\~~~~~~/              \n",
            "               \\            /          \n",
            "               \\         (              \n",
            "               \\___--    \\             \n",
            "             ,_  (      \\             \n",
            "          (  \\ |___` $    /|          \n",
            "              \\\\_( \"/ _/   \\\"   \\      \n",
            "             ,   `Y( .___.-|    \\_   \n",
            "               , \\\\``   `\"\"/   `.  \n",
            "             /   Y      \"   |    \\  \n",
            "             /  |  \\W   /   |\\   \n",
            "             |    |C   >  ,---()) \n",
            "             \\   \\\\       / /   \\ \n",
            "             '.  \\____.'  ()._,--,\n",
            "             ,'  `, () `-()     |\n",
            "jgs      `    `  //    `   `   `   \n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "üé® Pidiendo: 'sword'...\n",
            "----------------------------------------\n",
            "     .__        .oodb.  \n",
            "      /        \\ pradpsces :\n",
            "     /  \\   /   @@@P  \\   \n",
            "    .'  \\  /     @@@@@@`. \n",
            "   /       ~~.    `@@P  \\\n",
            "    /  /    ~~,    0@@@@@\\ \n",
            "   \\ |/-\\___/   '-@@@@@q-\\-\n",
            "     \\           _/@@@@@@) \n",
            "    \\|           /   @@b\n",
            "       \\   ,_,__/   @@@b\n",
            "       \\  _/ |       @@@L\n",
            "          `-====--\"       |\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "üé® Pidiendo: 'castle'...\n",
            "----------------------------------------\n",
            "          /|\\/\\ |\\     \n",
            "         \\/\\/|\\/\\/    \n",
            "     . | \\ | || \\|   \n",
            "     /  |/  ||\\/|   \n",
            "    /  |/|  ||    \n",
            "   :___|/___||    \n",
            "   \\/____\\/,-'-.  \n",
            "   (____( ccrys`! \n",
            "\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_ascii_pro(prompt_usuario, temperatura=0.6):\n",
        "    print(f\"\\nüé® Pidiendo: '{prompt_usuario}' (Temp: {temperatura})...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    start_prompt = f\"<|startoftext|>\\nUser: Dibujo ASCII de {prompt_usuario}\\nAI:\\n\"\n",
        "    context_idxs = [stoi.get(c, 0) for c in start_prompt]\n",
        "    context = torch.tensor([context_idxs], dtype=torch.long, device=device)\n",
        "\n",
        "    # AQU√ç EST√Å EL TRUCO: Dividimos los logits por la temperatura\n",
        "    # Si temp es baja (<1), exageramos las probabilidades altas (m√°s precisi√≥n)\n",
        "    # Si temp es alta (>1), igualamos probabilidades (m√°s caos)\n",
        "    for _ in range(1000):\n",
        "        idx_cond = context[:, -block_size:]\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / temperatura\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        context = torch.cat((context, idx_next), dim=1)\n",
        "\n",
        "        # Parada temprana si termina el dibujo\n",
        "        if idx_next.item() == stoi.get('<', 0): # Check r√°pido optimizado\n",
        "             if len(context[0]) > len(context_idxs) + 5: # M√≠nimo de dibujo\n",
        "                 break # Simplificaci√≥n para no procesar string todo el rato\n",
        "\n",
        "    resultado = ''.join([itos[i] for i in context[0].tolist()])\n",
        "    dibujo = resultado.replace(start_prompt, \"\").split(\"<|endoftext|>\")[0]\n",
        "    print(dibujo)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# PRUEBA ESTOS VALORES:\n",
        "generar_ascii_pro(\"sword\", temperatura=0.5)   # Deber√≠a ser m√°s recta\n",
        "generar_ascii_pro(\"castle\", temperatura=0.5)  # Muros m√°s s√≥lidos\n",
        "generar_ascii_pro(\"dragon\", temperatura=0.7)  # Un poco de creatividad para las alas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBFLr3koToGm",
        "outputId": "7fde3962-a305-4fcc-b609-1651617ff56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üé® Pidiendo: 'sword' (Temp: 0.5)...\n",
            "----------------------------------------\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
            "----------------------------------------\n",
            "\n",
            "üé® Pidiendo: 'castle' (Temp: 0.5)...\n",
            "----------------------------------------\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
            "----------------------------------------\n",
            "\n",
            "üé® Pidiendo: 'dragon' (Temp: 0.7)...\n",
            "----------------------------------------\n",
            "  _________________________________________________________________\n",
            " __________________________________________________________________________________\n",
            "__________________________________________________________________________\n",
            "                                                                           \n",
            "                                                                               \n",
            "                                                                                    \n",
            "                                                                                            \n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURACI√ìN PRO (Travian Mode) ---\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 10000      # ANTES: 5000 -> El doble de pr√°ctica\n",
        "learning_rate = 1e-4   # ANTES: 3e-4 -> Aprende m√°s lento pero con m√°s detalle\n",
        "\n",
        "# Aumentamos el tama√±o del cerebro\n",
        "n_embd = 384           # Igual\n",
        "n_head = 6             # Igual\n",
        "n_layer = 8            # ANTES: 6 -> M√°s capas para entender abstracci√≥n compleja\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "81zzFGTaTzcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Rutas\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "ruta_archivo = '/content/drive/MyDrive/ASCII-GPT/dataset/dataset_limpio_travian.txt'\n",
        "ruta_modelo = '/content/drive/MyDrive/ASCII-GPT/modelo_ascii_pro.pt' # <--- Nombre nuevo para no mezclar\n",
        "\n",
        "# 2. CONFIGURACI√ìN \"TRAVIAN MODE\" (M√°s potente)\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 10000      # 10.000 Pasos (M√°s entrenamiento)\n",
        "learning_rate = 1e-4   # Aprende m√°s despacio y con detalle\n",
        "eval_interval = 1000   # Te avisa cada 1000 pasos\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Aumentamos el tama√±o del cerebro (M√°s capas = M√°s inteligencia espacial)\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 8            # 8 Capas (Antes eran 6)\n",
        "dropout = 0.2\n",
        "\n",
        "print(f\"‚öôÔ∏è Iniciando Entrenamiento PRO en: {device}\")\n",
        "print(\"‚òï Esto tardar√° unos 30-40 minutos. Ve a por un caf√©.\")\n",
        "\n",
        "# 3. PREPARAR DATOS\n",
        "with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# 4. EL MODELO REFORZADO\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 5. LOOP DE ENTRENAMIENTO\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "print(\"\\nü•ä ¬°EMPIEZA EL ENTRENAMIENTO PRO!\")\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = {}\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for split in ['train', 'val']:\n",
        "                losses[split] = torch.zeros(200).to(device)\n",
        "                for k in range(200):\n",
        "                    X, Y = get_batch(split)\n",
        "                    logits, loss = model(X, Y)\n",
        "                    losses[split][k] = loss.item()\n",
        "                losses[split] = losses[split].mean()\n",
        "        model.train()\n",
        "        print(f\"Paso {iter}/{max_iters}: loss {losses['train']:.4f}\")\n",
        "\n",
        "        # Muestra visual\n",
        "        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "        print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Guardar modelo PRO\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'stoi': stoi,\n",
        "    'itos': itos,\n",
        "    'chars': chars\n",
        "}, ruta_modelo)\n",
        "print(f\"üíæ ¬°MODELO PRO GUARDADO! -> {ruta_modelo}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ_U7Y6aU43S",
        "outputId": "d97ca469-4586-4ca1-ab46-e6febe2aee81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è Iniciando Entrenamiento PRO en: cuda\n",
            "‚òï Esto tardar√° unos 30-40 minutos. Ve a por un caf√©.\n",
            "\n",
            "ü•ä ¬°EMPIEZA EL ENTRENAMIENTO PRO!\n",
            "Paso 0/10000: loss 4.8996\n",
            "\u0003zp&¬Øf4Y¬Ø4|^√©w;\"S√©7\n",
            "jj5]n!(Wv¬∞¬Ø=H #¬•?¬£√•tI'wgej3?D(Wzx9(X¬£;4zvK@ca5:p\"hTm\u0003¬Ø?√πV2Zn>/li~Uq^:{}g√£√ÖzwRLaF\n",
            "------------------------------\n",
            "Paso 1000/10000: loss 1.0259\n",
            "\u0003     &&&HHHHHHW:\n",
            " HHHHAMMHH| HHHHHHHHHHHHHHHVHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH|__\\|HHHHHHHHH  \n",
            "\n",
            "------------------------------\n",
            "Paso 2000/10000: loss 0.9623\n",
            "\u0003    .  . @^-..:o        \n",
            "                                -....        :..         .'.            :  \n",
            "------------------------------\n",
            "Paso 3000/10000: loss 0.8989\n",
            "\u0003o               / /           | |  ``   ;              \n",
            "   | )  |           `--------------'      `-\n",
            "------------------------------\n",
            "Paso 4000/10000: loss 0.8565\n",
            "\u0003TP   ##  I#  ##______________[ ##_____________________W0 \n",
            "2go981g.                    #A7A        ##\n",
            "------------------------------\n",
            "Paso 5000/10000: loss 0.8201\n",
            "\u00030p                         \n",
            "<|endoftext|>\n",
            "<|startoftext|>\n",
            "User: Dibujo ASCII de Art by Smarolons/sia\n",
            "------------------------------\n",
            "Paso 6000/10000: loss 0.7942\n",
            "\u0003    .'+ :  \n",
            "   '9$$$$$$$$$$$$$$$\n",
            "  :9 '9!':** *_            \n",
            "    '.___$$$$ :+'   \"         \n",
            " :$$$$$$\n",
            "------------------------------\n",
            "Paso 7000/10000: loss 0.7727\n",
            "\u0003;XXXX        __        XXXXXXXXXXX  \\             \n",
            "      XX  XXX  XXX/          XXXXXX    \\         \n",
            "------------------------------\n",
            "Paso 8000/10000: loss 0.7496\n",
            "\u0003H/\\|||              \n",
            " ..--..|   |,'.'      |.   .'              \n",
            "..-'   ''----'`-------'    .__..--\"\n",
            "------------------------------\n",
            "Paso 9000/10000: loss 0.7440\n",
            "\u0003ELOVELOVELOVELOVELOVEL   LOVELOVELOVEL   LOU\n",
            "LOVELOVE                   VV\t       VE   VELOVELO     \n",
            "------------------------------\n",
            "Paso 9999/10000: loss 0.7333\n",
            "\u0003c\n",
            "P.    d'       .     ,d88888888888b  `Ya,\n",
            "Y'      `Y'       .d88a.   .   .d88888888888888b   `Y  `\n",
            "------------------------------\n",
            "üíæ ¬°MODELO PRO GUARDADO! -> /content/drive/MyDrive/ASCII-GPT/modelo_ascii_pro.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Asegurar conexi√≥n\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# 2. Configuraci√≥n obligatoria (tiene que coincidir con el entrenamiento PRO)\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 8    # <--- IMPORTANTE: Esto es 8 porque usamos la versi√≥n PRO\n",
        "dropout = 0.2\n",
        "\n",
        "# DETECTOR AUTOM√ÅTICO DE CPU (Por si te quitaron la GPU)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"‚öôÔ∏è Trabajando con: {device} (Si es CPU, tardar√° unos segundos m√°s, paciencia)\")\n",
        "\n",
        "# 3. Reconstruir la arquitectura del cerebro\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits, None\n",
        "\n",
        "# 4. CARGAR EL MODELO PRO\n",
        "ruta_modelo = '/content/drive/MyDrive/ASCII-GPT/modelo_ascii_pro.pt'\n",
        "\n",
        "if os.path.exists(ruta_modelo):\n",
        "    print(f\"üìÇ Cargando cerebro PRO desde: {ruta_modelo}\")\n",
        "    # map_location es la magia para que funcione sin GPU\n",
        "    checkpoint = torch.load(ruta_modelo, map_location=torch.device(device))\n",
        "\n",
        "    stoi = checkpoint['stoi']\n",
        "    itos = checkpoint['itos']\n",
        "    chars = checkpoint['chars']\n",
        "    vocab_size = len(chars)\n",
        "\n",
        "    model = GPTLanguageModel(vocab_size)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ ¬°Modelo PRO cargado exitosamente!\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: No encuentro el archivo modelo_ascii_pro.pt. Revisa tu Drive.\")\n",
        "\n",
        "# 5. GENERADOR INTELIGENTE\n",
        "def dibujar(prompt, temp=0.6):\n",
        "    print(f\"\\nüé® Prompt: '{prompt}' (Temp: {temp})...\")\n",
        "    start = f\"<|startoftext|>\\nUser: Dibujo ASCII de {prompt}\\nAI:\\n\"\n",
        "    idx = torch.tensor([[stoi.get(c, 0) for c in start]], dtype=torch.long, device=device)\n",
        "\n",
        "    # Generamos caracter por caracter\n",
        "    # Si usas CPU, esto ir√° un poco lento, ver√°s como escribe poco a poco\n",
        "    generated = idx\n",
        "    for _ in range(1500): # Max tokens\n",
        "        with torch.no_grad():\n",
        "            logits, _ = model(generated[:, -block_size:])\n",
        "        logits = logits[:, -1, :] / temp\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_idx = torch.multinomial(probs, num_samples=1)\n",
        "        generated = torch.cat((generated, next_idx), dim=1)\n",
        "\n",
        "        # Parada si detecta fin\n",
        "        if next_idx.item() == stoi.get('<', 0): # Optimizaci√≥n simple\n",
        "            decoded = ''.join([itos[i] for i in generated[0].tolist()])\n",
        "            if \"<|endoftext|>\" in decoded:\n",
        "                break\n",
        "\n",
        "    final_text = ''.join([itos[i] for i in generated[0].tolist()])\n",
        "    clean_art = final_text.replace(start, \"\").split(\"<|endoftext|>\")[0]\n",
        "    print(clean_art)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# --- ZONA DE PRUEBAS ---\n",
        "# Prueba varias cosas para ver qu√© aprendi√≥ mejor\n",
        "dibujar(\"dragon\", 0.6)\n",
        "dibujar(\"castle\", 0.5) # Baja temperatura para castillos (m√°s recto)\n",
        "dibujar(\"sword\", 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4MnI5XfQZDI",
        "outputId": "b44ec55a-311e-4bbd-d30a-c3ba783234d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚öôÔ∏è Trabajando con: cpu (Si es CPU, tardar√° unos segundos m√°s, paciencia)\n",
            "üìÇ Cargando cerebro PRO desde: /content/drive/MyDrive/ASCII-GPT/modelo_ascii_pro.pt\n",
            "‚úÖ ¬°Modelo PRO cargado exitosamente!\n",
            "\n",
            "üé® Prompt: 'dragon' (Temp: 0.6)...\n",
            "                                                                                                  \n",
            "                                                                                                                       \n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
            "--------------------------------------------------\n",
            "\n",
            "üé® Prompt: 'castle' (Temp: 0.5)...\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
            "--------------------------------------------------\n",
            "\n",
            "üé® Prompt: 'sword' (Temp: 0.5)...\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "# ... (Asumimos que las clases y el modelo ya est√°n cargados del paso anterior)\n",
        "# Si te da error de \"name 'model' is not defined\", vuelve a ejecutar el bloque anterior.\n",
        "\n",
        "print(\"üïµÔ∏è‚Äç‚ôÇÔ∏è INICIANDO MODO DIAGN√ìSTICO (RAYO-X)...\")\n",
        "\n",
        "def dibujar_con_rayos_x(prompt, temp=0.8):\n",
        "    print(f\"\\nüß™ Pidiendo: '{prompt}'\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "    # Formato exacto del entrenamiento\n",
        "    start = f\"<|startoftext|>\\nUser: Dibujo ASCII de {prompt}\\nAI:\\n\"\n",
        "\n",
        "    # Verificamos si entiende todas las letras del prompt\n",
        "    unknowns = [c for c in start if c not in stoi]\n",
        "    if unknowns:\n",
        "        print(f\"‚ö†Ô∏è ALERTA: El modelo NO conoce estos caracteres: {unknowns}\")\n",
        "        print(\"Esto confundir√° a la IA. Intenta usar solo letras que estaban en el dataset.\")\n",
        "\n",
        "    idx = torch.tensor([[stoi.get(c, 0) for c in start]], dtype=torch.long, device=device)\n",
        "\n",
        "    generated = idx\n",
        "    print(\"üß† Pensando (Caracter a caracter):\")\n",
        "\n",
        "    # Generamos solo 200 caracteres para probar r√°pido\n",
        "    for i in range(200):\n",
        "        with torch.no_grad():\n",
        "            logits, _ = model(generated[:, -block_size:])\n",
        "\n",
        "        logits = logits[:, -1, :] / temp\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_idx = torch.multinomial(probs, num_samples=1)\n",
        "        generated = torch.cat((generated, next_idx), dim=1)\n",
        "\n",
        "        # Decodificar el token nuevo\n",
        "        char_nuevo = itos[next_idx.item()]\n",
        "\n",
        "        # MOSTRAR LO QUE PIENSA EN TIEMPO REAL\n",
        "        if char_nuevo == '\\n':\n",
        "            print(\"‚Üµ\") # S√≠mbolo visual para Enter\n",
        "        elif char_nuevo == ' ':\n",
        "            print(\"¬∑\", end='') # Puntito para espacio\n",
        "        else:\n",
        "            print(char_nuevo, end='') # Caracter normal\n",
        "\n",
        "        sys.stdout.flush() # Forzar que se imprima ya\n",
        "\n",
        "        # Parar si termina\n",
        "        if \"<|endoftext|>\" in char_nuevo: # A veces se parte en tokens\n",
        "            break\n",
        "\n",
        "    print(\"\\n--------------------------------------------------\")\n",
        "    print(\"üèÅ Diagn√≥stico terminado.\")\n",
        "\n",
        "# PRUEBA ESTO:\n",
        "# 1. Usamos una temperatura m√°s alta (0.8) para que se arriesgue a pintar algo\n",
        "# 2. Probamos palabras que SEGURO estaban en el dataset (en ingl√©s)\n",
        "dibujar_con_rayos_x(\"dragon\", temp=0.8)\n",
        "dibujar_con_rayos_x(\"sword\", temp=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxdkLU6FbbTD",
        "outputId": "b1964484-66ed-41da-b23d-9a2e371078e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üïµÔ∏è‚Äç‚ôÇÔ∏è INICIANDO MODO DIAGN√ìSTICO (RAYO-X)...\n",
            "\n",
            "üß™ Pidiendo: 'dragon'\n",
            "--------------------------------------------------\n",
            "üß† Pensando (Caracter a caracter):\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑.--.¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑‚Üµ\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\\¬∑`.¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑‚Üµ\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑|¬∑¬∑¬∑\\¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑‚Üµ\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑|¬∑¬∑¬∑\\¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "--------------------------------------------------\n",
            "üèÅ Diagn√≥stico terminado.\n",
            "\n",
            "üß™ Pidiendo: 'sword'\n",
            "--------------------------------------------------\n",
            "üß† Pensando (Caracter a caracter):\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑______¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑‚Üµ\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑___¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑|¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑‚Üµ\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑|¬∑\\¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑_____|¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑‚Üµ\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑|¬∑¬∑¬∑\\¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑(¬∑¬∑¬∑¬∑\n",
            "--------------------------------------------------\n",
            "üèÅ Diagn√≥stico terminado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Aseg√∫rate de tener 'model', 'stoi', 'itos' y 'device' cargados del paso anterior\n",
        "\n",
        "def dibujar_agresivo(prompt, penalizacion_espacio=1.2, temp=0.9):\n",
        "    print(f\"\\nüé® Prompt AGRESIVO: '{prompt}'\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "    start_str = f\"<|startoftext|>\\nUser: Dibujo ASCII de {prompt}\\nAI:\\n\"\n",
        "    idx = torch.tensor([[stoi.get(c, 0) for c in start_str]], dtype=torch.long, device=device)\n",
        "\n",
        "    # Buscamos cu√°l es el ID del espacio para castigarlo\n",
        "    id_espacio = stoi.get(' ', -1)\n",
        "\n",
        "    generated = idx\n",
        "    conteo_espacios = 0\n",
        "\n",
        "    # Generamos 1000 tokens\n",
        "    for _ in range(1000):\n",
        "        with torch.no_grad():\n",
        "            logits, _ = model(generated[:, -block_size:])\n",
        "\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # --- AQU√ç EST√Å EL TRUCO ---\n",
        "        # Si el token del espacio existe, le restamos probabilidad (logit)\n",
        "        if id_espacio != -1:\n",
        "            logits[0, id_espacio] = logits[0, id_espacio] / penalizacion_espacio\n",
        "\n",
        "        # Aplicamos temperatura alta para que sea creativa\n",
        "        logits = logits / temp\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        next_idx = torch.multinomial(probs, num_samples=1)\n",
        "        generated = torch.cat((generated, next_idx), dim=1)\n",
        "\n",
        "        # Check fin\n",
        "        if next_idx.item() == stoi.get('<', 0):\n",
        "             decoded = ''.join([itos[i] for i in generated[0][-10:].tolist()])\n",
        "             if \"<|endoftext|>\" in decoded:\n",
        "                 break\n",
        "\n",
        "    # Mostrar resultado\n",
        "    full_text = ''.join([itos[i] for i in generated[0].tolist()])\n",
        "    clean_art = full_text.replace(start_str, \"\").split(\"<|endoftext|>\")[0]\n",
        "    print(clean_art)\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "# PROBAR CON DISTINTOS NIVELES DE AGRESI√ìN\n",
        "# penalizacion_espacio > 1.0 hace que los espacios sean menos probables\n",
        "dibujar_agresivo(\"dragon\", penalizacion_espacio=1.5, temp=0.9)\n",
        "dibujar_agresivo(\"castle\", penalizacion_espacio=2.0, temp=1.0) # Muy agresivo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79tpFv62dA8i",
        "outputId": "f0d5ca69-f930-4e18-cb71-746f6d49e643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üé® Prompt AGRESIVO: 'dragon'\n",
            "--------------------------------------------------\n",
            "        ._.  \n",
            " . |~~~|_. \n",
            "|. |~~.~~\\.\n",
            "|~~|~~.~|~~|\n",
            "|~~~|~|~|~|~~|\n",
            "/|~|~|~~|~~|~|\n",
            "|~~|~|~~|~~|~~~|\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üé® Prompt AGRESIVO: 'castle'\n",
            "--------------------------------------------------\n",
            ",||||||||,>\n",
            "|||||||||\n",
            "|||||||||\n",
            "||||`.`'.`'.`'.`'.|\\\n",
            "|||`'||\"`'`'\\||.`'`'||\n",
            "||||'''``.`'`'`'|||\n",
            "||''`'`'``'---'|||\n",
            "||''`'`'+`'`'''``'`'`||\n",
            "||#|/|##/|##|#||#\\#|||\n",
            "||^^^'`'``^'#^`'^||||\n",
            "||,`^^`'`'`'|||#||#|\n",
            "|||###^``^^',|||####||\n",
            "||#\\^^^^^^^^'|||####||\n",
            "||`^^'^^\"''`^'|||##|###||\n",
            "||^^^`^^'^`^^'||||######||\n",
            "~^^^^^^^^^^^^^^^^^^^^^^^^^^^||\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}